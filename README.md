# ğŸ“Œ Credit Card Fraud Detection  
[![Python](https://img.shields.io/badge/Python-3.10-blue.svg)]()  
[![Notebook](https://img.shields.io/badge/Jupyter-Notebook-orange.svg)]()  
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)]()  

An end-to-end machine learning project for detecting fraudulent credit card transactions.  
The goal is to build a complete fraud detection workflow including:

- Data preprocessing  
- Feature engineering  
- Handling severe class imbalance (SMOTE)  
- Model training (Logistic Regression, Random Forest, XGBoost)  
- Threshold tuning  
- Model comparison using fraud-focused metrics (PR-AUC, recall, F1)

---

## ğŸ“‚ Repository Structure

```text
Fraud-Detection/
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ 01_credit_card_fraud.ipynb
â”‚
â”œâ”€â”€ src/
â”‚   â””â”€â”€ .keep
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â””â”€â”€ LICENSE
```

---

## ğŸ“Š Dataset

**Source:** Kaggle â€” Credit Card Fraud Detection  
**Rows:** 284,807  
**Fraud cases:** 492 (0.17%) â†’ extremely imbalanced  

Features:
- `V1`â€¦`V28` â†’ anonymized PCA components  
- `Amount` â†’ transaction value  
- `Time` â†’ seconds elapsed from a reference point  
- `Class` â†’ target variable (0 = legitimate, 1 = fraud)

---

## ğŸ”§ Machine Learning Pipeline

### **1ï¸âƒ£ Data Cleaning**
- Checked for missing values (none found)  
- Removed duplicates  
- Standardized numerical features where needed  

### **2ï¸âƒ£ Feature Engineering**
Created additional features:
- Hour of transaction  
- Part of day (morning, afternoon, evening, night)  
- Log-transformed transaction amount  
- Dropped `Time` after extracting useful patterns  

### **3ï¸âƒ£ Handling Class Imbalance**
Used **SMOTE** to oversample minority class after splitting.

### **4ï¸âƒ£ Train/Test Split**
- 80/20 split  
- Stratified to preserve fraud ratio  

### **5ï¸âƒ£ Models Used**
- Logistic Regression  
- Random Forest Classifier  
- XGBoost Classifier  

### **6ï¸âƒ£ Threshold Tuning**
Optimized XGBoost decision threshold using the **Precision-Recall Curve** to balance recall and precision.

---

## ğŸ“ˆ Model Performance Comparison

| Model | Accuracy | Precision | Recall | F1 | ROC-AUC | PR-AUC |
|------|----------|-----------|--------|-----|----------|---------|
| Logistic Regression | 0.9738 | 0.053 | 0.874 | 0.100 | 0.9587 | 0.6849 |
| Random Forest | 0.9995 | 0.913 | 0.768 | 0.834 | 0.9687 | 0.8205 |
| XGBoost | 0.9988 | 0.611 | 0.811 | 0.697 | 0.9780 | 0.8083 |
| **XGBoost (Tuned)** | **0.9995** | **0.924** | **0.768** | **0.839** | **0.9780** | **0.8083** |

### â­ **Best overall model:**  
**XGBoost with threshold tuning** â†’ highest precision while maintaining strong recall.

---

## ğŸ“‰ Confusion Matrices  
The notebook includes heatmaps for:

- Logistic Regression  
- Random Forest  
- XGBoost  
- XGBoost (Tuned Threshold)  

These visuals help understand trade-offs between false positives and false negatives.

---

## ğŸ§  What I Learned

- How to handle extremely imbalanced datasets  
- Correct use of SMOTE (after train/test split)  
- Why accuracy is misleading in fraud detection  
- How to tune classification thresholds  
- How to compare models using PR-AUC and F1  
- Structuring a full ML pipeline end-to-end  
- Building a clean, reproducible GitHub project  

---
